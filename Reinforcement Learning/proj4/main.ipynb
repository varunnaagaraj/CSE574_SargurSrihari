{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wpdffOrctdb_"
   },
   "source": [
    "# Project 4 - Reinforcement Learning\n",
    "### Deadline - December 5, 11.59pm\n",
    "\n",
    "Welcome to your last assignment for CSE4/574 course! For this assignment you will implement a reinforcement learing algorithm - DQN, that will train the agent to play a game.\n",
    "\n",
    "All the code deliverables has to be provided within this notebook.\n",
    "\n",
    "## 1 - Packages\n",
    "\n",
    "Let's first import all the packages that you will need during this assignment.\n",
    "\n",
    "-  [random](https://docs.python.org/3/library/random.html) - generates pseudo-random numbers\n",
    "-  [math](https://docs.python.org/3/library/math.html?highlight=math#module-math) - provides access to the a big variety of mathematical functions\n",
    "-  [time](https://docs.python.org/3/library/time.html?highlight=time#module-time) - will be used to track how much time each computation takes\n",
    "- [numpy](http://www.numpy.org/) -  is the main package for scientific computing with Python\n",
    "- [keras](https://keras.io/) -  is a high-level neural networks API, we will use to biuild a neural network for our agent\n",
    "- [matplotlib](https://matplotlib.org/) - is a plotting library\n",
    "- [IPython](https://ipython.org/) - is an interactive shell, that will help us to display our framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "io_a8t4stK-u",
    "outputId": "46034652-67eb-4461-df85-c58d4458ee0c"
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Authors:                                                            #\n",
    "# Nathan Margaglio (nathanmargaglio@gmail.com)                        #\n",
    "# Alina Vereshchaka (avereshc@buffalo.edu)                            #\n",
    "#######################################################################\n",
    "\n",
    "import random, math, time\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from matplotlib import rc, animation\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "  from google.colab import files\n",
    "except:\n",
    "  print(\"Could not import Google Colab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbHAa4dlpxCO"
   },
   "source": [
    "## 2 - Environment\n",
    "Here we define our grid-world environment.\n",
    "No need to make any changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzzGQ1u7tOIc"
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "  def __init__(self, grid_size):\n",
    "      self.grid_size = grid_size\n",
    "      \n",
    "      self.cat = imread('https://image.ibb.co/btGeAA/tom.png')\n",
    "      self.mouse = imread('https://image.ibb.co/njNNxq/jerry.png')\n",
    "      self.confetti = imread('https://image.ibb.co/ganuAA/tom-and-jerry.png')\n",
    "      self.dim = 1.5\n",
    "      \n",
    "      self.rewards = []\n",
    "      \n",
    "  def _update_state(self, action):\n",
    "      state = self.state\n",
    "      # 0 = left\n",
    "      # 1 = right\n",
    "      # 2 = down\n",
    "      # 3 = up\n",
    "\n",
    "      fy, fx, py, px = state\n",
    "      old_d = abs(fx - px) + abs(fy - py)\n",
    "\n",
    "      if action == 0:\n",
    "          if px > 0:\n",
    "              px -= 1\n",
    "      if action == 1:\n",
    "          if px < self.grid_size-1:\n",
    "              px += 1\n",
    "      if action == 2:\n",
    "          if py > 0:\n",
    "              py-= 1\n",
    "      if action == 3:\n",
    "          if py < self.grid_size-1:\n",
    "              py += 1\n",
    "\n",
    "      new_d = abs(fx - px) + abs(fy - py)\n",
    "      self.d = old_d-new_d\n",
    "      self.time = self.time - 1\n",
    "      return np.array([fy, fx, py, px])\n",
    "\n",
    "  def _get_reward(self):\n",
    "    fruit_y, fruit_x, player_y, player_x = self.state\n",
    "    if fruit_x == player_x and fruit_y == player_y: return 1\n",
    "    if self.d == 1: return 1\n",
    "    if self.d == 0: return -1\n",
    "    if self.d == -1: return -1\n",
    "\n",
    "  def _is_over(self):\n",
    "    fruit_y, fruit_x, player_y, player_x = self.state\n",
    "    if self.time == 0: return True\n",
    "    if fruit_x == player_x and fruit_y == player_y: return True\n",
    "    return False\n",
    "\n",
    "  def step(self, action):\n",
    "    self.state = self._update_state(action)\n",
    "    reward = self._get_reward()\n",
    "    self.rewards.append(reward)\n",
    "    game_over = self._is_over()\n",
    "    return self.state, reward, game_over\n",
    "  \n",
    "  def render(self):\n",
    "    # Note: there's no promises of efficieny with this method\n",
    "    # If things are slow, remove it\n",
    "    \n",
    "    im_size = (self.grid_size,)*2\n",
    "    state = self.state\n",
    "    \n",
    "    self.fig = plt.figure(figsize=(8, 6), dpi=80)\n",
    "    self.ax = self.fig.add_subplot(111)\n",
    "    \n",
    "    self.ax.clear()\n",
    "    self.ax.set_ylim((-1, self.grid_size))\n",
    "    self.ax.set_xlim((-1, self.grid_size))\n",
    "    #self.ax.axis('off') # uncomment to turn off axes\n",
    "    self.ax.get_xaxis().set_ticks(range(self.grid_size))\n",
    "    self.ax.get_yaxis().set_ticks(range(self.grid_size))\n",
    "    \n",
    "    xc = state[2]\n",
    "    yc = state[3]\n",
    "    xm = state[0]\n",
    "    ym = state[1]\n",
    "    \n",
    "    if state[0] == state[2] and state[1] == state[3]:\n",
    "      self.ax.imshow(self.confetti, \n",
    "                     extent=(-1, self.grid_size,\n",
    "                             -1, self.grid_size))\n",
    "    else:\n",
    "      self.ax.imshow(self.mouse, \n",
    "                     extent=(xm-self.dim/4, xm+self.dim/4,\n",
    "                             ym-self.dim/4, ym+self.dim/4))\n",
    "      self.ax.imshow(self.cat, \n",
    "                     extent=(xc-self.dim/2, xc+self.dim/2,\n",
    "                             yc-self.dim/2, yc+self.dim/2))\n",
    "    self.fig.canvas.draw()\n",
    "    return np.array(self.fig.canvas.renderer._renderer)\n",
    "\n",
    "  def reset(self, deterministic=True):\n",
    "    if deterministic:\n",
    "      # this is an easier environment setup\n",
    "      fruit_x = 0\n",
    "      fruit_y = 0\n",
    "      player_x = self.grid_size - 1\n",
    "      player_y = self.grid_size - 1\n",
    "      time = self.grid_size*2\n",
    "    else:\n",
    "      generated = False\n",
    "      while not generated\\\n",
    "      or abs(fruit_x - player_x) + abs(fruit_y - player_y) < self.grid_size/2:\n",
    "        fruit_x = np.random.randint(0, self.grid_size-1)\n",
    "        fruit_y = np.random.randint(0, self.grid_size-1)\n",
    "        player_x = np.random.randint(0, self.grid_size-1)\n",
    "        player_y = np.random.randint(0, self.grid_size-1)\n",
    "        time = abs(fruit_x - player_x) + abs(fruit_y - player_y)\n",
    "        time *= 2\n",
    "        generated = True\n",
    "\n",
    "    self.time = time\n",
    "    self.d = 0\n",
    "    self.state = np.asarray([fruit_y, fruit_x, player_y, player_x])\n",
    "\n",
    "    return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7Urs08uH78l"
   },
   "source": [
    "### Random actions\n",
    "This runs the environment using random actions. Try to run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNFJqPD4N3xm"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This runs the environment using random actions\n",
    "\"\"\"\n",
    "\n",
    "print('Setting up environment')\n",
    "env = Environment(5)\n",
    "num_episodes = 1 # number of games we want the agent to play\n",
    "env.reset()\n",
    "frames = []\n",
    "RENDER = True\n",
    "print('Running random simulation')\n",
    "for episode in range(num_episodes):\n",
    "  print('Resetting environment')\n",
    "  s = env.reset() # Initial state\n",
    "  while True: \n",
    "    a = np.random.choice(range(4)) # choose a random action\n",
    "    s_, r, done = env.step(a) # apply random action\n",
    "    \n",
    "    if RENDER:\n",
    "      fig = env.render()\n",
    "      plt.imshow(fig)\n",
    "      plt.show()\n",
    "      frames.append(fig)\n",
    "\n",
    "    if done:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rDHMlgTaljVk"
   },
   "source": [
    "## 3 - Brain\n",
    "\n",
    "The 'brain' of the agent is where the model is created and held.\n",
    "\n",
    "### Neural Network structure for our task\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1KsdRkIp9_w-wdTDMYANAO6BEdXdllYAC\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "\n",
    "\n",
    "Our DQN takes a stack of six-tuple as an input. It is passed through two hidden networks, and output a vector of Q-values for each action possible in the given state. \\\\\n",
    "** Example: **\n",
    "$Q(s_t, a_1)$ - q-value for a given state $s$, if we choose action $a_1$ \\\\\n",
    "We need to choose such an action, that will return the higest Q-value.\n",
    "\n",
    "In the beginning, the agent does really badly. But over time, it begins to associate states with best actions to do.\n",
    " \n",
    " \n",
    "###  <font color='red'>Task 1: Build a 3-layer neural network, using Keras liblary.</font>  <br>\n",
    "**Instructions:**\n",
    "- Build a three-layer neural network with two hidden layers\n",
    "- The model's structure is: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR.\n",
    "- Activation function for the first and second hidden layers is <mark>'relu'</mark>\n",
    "- Activation function for the final layer is  <mark>'linear'</mark> (that returns real values)\n",
    "- Input dimentions for the first hidden layer equals to the size of your observation space (<mark>state_dim</mark>)\n",
    "- Number of hidden nodes is <mark>128</mark>\n",
    "- Number of the output should be the same as the size of the action space (<mark>action_dim</mark>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1czfGeThfWis"
   },
   "outputs": [],
   "source": [
    "#-------------------- BRAIN ---------------------------\n",
    "\n",
    "class Brain:\n",
    "  \"\"\"The 'brain' of the agent, where the model is created and held.\n",
    "  \n",
    "  state_dim (int): the size of the observation space\n",
    "  action_dim (int): the size of the action space\n",
    "  \n",
    "  \"\"\"\n",
    "  def __init__(self, state_dim, action_dim, weights=None):\n",
    "    self.state_dim = state_dim\n",
    "    self.action_dim = action_dim\n",
    "\n",
    "    self.model = self._createModel()\n",
    "    if weights:\n",
    "      self.model.load_weights(\"brain.h5\")\n",
    "\n",
    "  def _createModel(self):\n",
    "    # Creates a Sequential Keras model\n",
    "    # This acts as the Deep Q-Network (DQN)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    model.add(Dense(output_dim=128, activation='relu', input_dim=self.state_dim))\n",
    "    model.add(Dense(output_dim=128, activation='relu'))\n",
    "    model.add(Dense(output_dim=self.action_dim, activation='linear'))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    opt = RMSprop(lr=0.00025)\n",
    "    model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "    return model\n",
    "\n",
    "  def train(self, x, y, epoch=1, verbose=0):\n",
    "    self.model.fit(x, y, batch_size=64, nb_epoch=epoch, verbose=verbose)\n",
    "\n",
    "  def predict(self, s):\n",
    "    return self.model.predict(s)\n",
    "\n",
    "  def predictOne(self, s):\n",
    "    return self.predict(s.reshape(1, self.state_dim)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YVD3qUwcozM1"
   },
   "source": [
    "## 4 - Memory\n",
    "\n",
    "In this block we are defining the main functions that will be used to store the exeriences of our agent. <br>\n",
    "No need to make any modifications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Uq8Aen6fZUD"
   },
   "outputs": [],
   "source": [
    "#-------------------- MEMORY --------------------------\n",
    "class Memory:   # stored as ( s, a, r, s_ )\n",
    "  \"\"\"The agent's 'memory', where experiences are stored\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self.capacity = capacity\n",
    "    self.samples = []\n",
    "\n",
    "  def add(self, sample):\n",
    "    # a sample should be an array [s, a, r, s_]\n",
    "    # s: current state\n",
    "    # a: current action\n",
    "    # r: current reward\n",
    "    # s_: next state\n",
    "    self.samples.append(sample)        \n",
    "\n",
    "    if len(self.samples) > self.capacity:\n",
    "        self.samples.pop(0)\n",
    "\n",
    "  def sample(self, n):\n",
    "    n = min(n, len(self.samples))\n",
    "    return random.sample(self.samples, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAFiQY7ApMyT"
   },
   "source": [
    "## 5 - Agent\n",
    "\n",
    "[np.amax](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.amax.html) - \n",
    "Returns the maximum of an array \n",
    "\n",
    "### Epsilon\n",
    "\n",
    "Our agent will randomly select its action at first by a certain percentage, called ‘exploration rate’ or ‘epsilon’. This is because at first, it is better for the agent to try all kinds of things before it starts to see the patterns. When it is not deciding the action randomly, the agent will predict the reward value based on the current state and pick the action that will give the highest reward. We want our agent to descrese the number of random action, as it goes, so we indroduce an exponential-decay epsilon, that eventually will allow our agent to explore the evironment. \\\\\n",
    "\n",
    "**Exponential-decay formula for epsilon:** \\\\\n",
    "\n",
    "\\begin{equation}\n",
    "\\epsilon = \\epsilon_{min} + (\\epsilon_{max} - \\epsilon_{min})*e^{-\\lambda|S|},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\epsilon_{min}, \\epsilon_{max} \\in [0, 1]$ \\\\\n",
    "$\\lambda$ - hyperparameter for epsilon \\\\\n",
    "$|S|$ - total number of steps\n",
    "\n",
    "\n",
    "###  <font color='red'>Task 2: Implement exponential-decay formula for epsilon.</font>  <br>\n",
    "**Instructions:**\n",
    "- On line 50 implement the formula provided above to calculate epsilon.\n",
    "- Please note, that the name for all the variables should start with <mark>self</mark>, thus </br> \n",
    "\n",
    "epsilon $\\rightarrow$ self.epsilon </br> \n",
    "min_epsilon $\\rightarrow$ self.min_epsilon\n",
    "\n",
    "###  <font color='red'>Task 3: Implement Q-function</font>  <br>\n",
    "**Instructions:**\n",
    "            \\begin{align} \\notag\n",
    "            Q_t = \\begin{cases} r_t, & \\mbox{if episode terminates at step } t+1 \\\\ r_t + \\gamma max_aQ(s_t, a_t; \\Theta), & \\mbox{otherwise} \\end{cases}\n",
    "            \\end{align} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nxd6viX_tiCu"
   },
   "outputs": [],
   "source": [
    "# -------------------- AGENT ---------------------------\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"The agent, which learns to navigate the environment\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, memory_capacity=10000,\n",
    "                 batch_size=64, gamma=0.99, lamb=0.001,\n",
    "                 max_epsilon=1., min_epsilon=0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma  # discount rate, to calculate the future discounted reward\n",
    "        self.lamb = lamb\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "        self.brain = Brain(state_dim, action_dim)\n",
    "        self.memory = Memory(memory_capacity)\n",
    "        self.steps = 0\n",
    "        self.epsilons = []\n",
    "\n",
    "    def act(self, s, verbose=False):\n",
    "        \"\"\"The policy of the agent:\n",
    "        Here, we determine if we explore (take a random action) based on epsilon.\n",
    "        If not, we have the model predict the Q-Values for the state,\n",
    "        then take the action which maximizes those values.\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            if verbose:\n",
    "                print(\"Random Action.\")\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            actions = self.brain.predictOne(s)\n",
    "            if verbose:\n",
    "                print(\"Actions:\", actions)\n",
    "            return np.argmax(actions)\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        \"\"\"The agent observes an event.\n",
    "        We pass a sample (state, action, reward, next state) to be stored in memory.\n",
    "        We then increment the step count and adjust epsilon accordingly.\n",
    "        \"\"\"\n",
    "        self.memory.add(sample)\n",
    "\n",
    "        # slowly decrease Epsilon based on our experience\n",
    "        self.steps += 1\n",
    "\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * math.exp(-self.lamb * self.steps)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        self.epsilons.append(self.epsilon)\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"The agent learns based on previous experiences.\n",
    "        We sample observations (state, action, reward, next state) from memory.\n",
    "        We train the model based on these observations.\n",
    "        \"\"\"\n",
    "\n",
    "        # Random sample of experiences\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        batch_size = len(batch)\n",
    "\n",
    "        # Extracting states ('current' and 'next') from samples\n",
    "        no_state = np.zeros(self.state_dim)\n",
    "        states = np.array([o[0] for o in batch])\n",
    "        states_next = np.array([(no_state if o[3] is None else o[3]) for o in batch])\n",
    "\n",
    "        # Estimating Q-Values for states\n",
    "        q_vals = self.brain.predict(states)\n",
    "        q_vals_next = self.brain.predict(states_next)\n",
    "\n",
    "        # Setting up training data\n",
    "        x = np.zeros((batch_size, self.state_dim))\n",
    "        y = np.zeros((batch_size, self.action_dim))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Observation\n",
    "            obs = batch[i]\n",
    "\n",
    "            # State, Action, Reward, Next State\n",
    "            st = obs[0];\n",
    "            act = obs[1];\n",
    "            rew = obs[2];\n",
    "            st_next = obs[3]\n",
    "\n",
    "            # Estimated Q-Values for Observation\n",
    "            t = q_vals[i]\n",
    "\n",
    "            ### START CODE HERE ### (≈ 4 line of code)\n",
    "            t[act] = rew if st_next is None else rew + self.gamma * np.amax(q_vals_next[i])\n",
    "\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            # Set training data\n",
    "            x[i] = st\n",
    "            y[i] = t\n",
    "\n",
    "        # Train\n",
    "        self.brain.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMwl9zrRp0J8"
   },
   "source": [
    "## 6 - Running the game\n",
    "\n",
    "### Environment\n",
    "First, we initialize our environment.  The environment, loosely structured like [OpenAI's Gym Environments](https://gym.openai.com/), has three main methods: `reset`, `step` and `render`.\n",
    "\n",
    "- When we call **reset**, we initialize the environment with a fresh episode. This allows us to effectively run through episodes (only needing to call reset at the beginning of an episode), but, more importantly, `reset()` returns the environment's initial state.\n",
    "\n",
    "- The **step** method accepts an action as a parameter (which, for this example, is an integer in [0, 3]), processes the action, and returns the new state, the reward for performing the action, and a boolean indicating if the run is over.\n",
    "\n",
    "- The **render** method simplay displays the state in a \"human-readable\" way. In this example, it renders an image of the environment as well as diagnostic data. This method is solely used for debugging purposes, and can be omitted to speed up training time.\n",
    "\n",
    "### Agent\n",
    "When we initialize the agent, we must pass both a `state_dim` and `action_dim` into it's constructor.  These values tell the agent the dimensions of the input and the output of the neural network.  The agent has three main methods: `act`, `observe`, and `replay`.\n",
    "\n",
    "- The **act** method takes the current state as input and returns the agent's selected action.  Optionally, you can pass a boolean to it's `verbose` parameter to print the resulting Q-Values.  Within the method, we first check if we should choose a random action (in order to explore) by comparing a randomly generated number to epsilon. If the agent decides to return a random action, then it's simply a randomly selected action from the action space (in this case, an integer in [0,3]).\n",
    "\n",
    "  If the agent doesn't choose a random action, then the state is passed to the agent's neural network (i.e., it's `Brain` object).  This results in an array of expected discounted rewards corresponding to each action (so, for example, if the resulting array is `[0.1, 2.3, 1.5, -0.7]`, then this means it expects choosing action `0` to lead to discounted rewards of `0.1`, action `1` leading to `2.3`, etc.).  Since we are greedy with our choices, we merely choose the action corresponding to the largest reward.\n",
    "\n",
    "- The **observe** method recieves an observation tuple `(s, a, r, s_)` as input and doesn't return anything.  Here, the observation tuple contains the current state `s`, the agent's action `a` at the current state, the reward `r` recieved for taking the action at the current state, and the resulting state `s_` that occurs after taking the action at the current state.  This tuple is saved to the agent's `Memory`, which acts as a simple queue.\n",
    "\n",
    "- The **replay** method is where any actual learning occurs.  Up to this point, we haven't trained the agent's neural network, only applied it to determine actions given states.  In order to train it, we implement **Experience Replay**, which, in short, allows the agent to not only learn from recent observations but also previous observations.  During experinece replay, we randomly sample a set of observations from the agent's `Memory`.  These observations are then constructed in a way that allows us to pass them through the neural network to train it.  \n",
    "\n",
    "  Simply put: after we make a complete observation, we have generated a training example.  The input for the example is the state, and the output is the expected discounted reward for each action.  In this way, training works the same way it would in any classical deep learning task (except our data is generated on the fly instead of collected prior to training).\n",
    "\n",
    "### Algorthm\n",
    "\n",
    "The algorthm, implemented below, simply calls these methods in sequence for some given number of episodes.  At the beginning of an episode, we reset the environment, and pass it's return value, the initial state, to the agent's act method.  This returns an action, which is then passed to the environment's step method.  This returns the next state, the reward, and the boolean indicating if the episode is over.  We then save the observation tuple to the agent's memory via the agent's observe method, then run a round of training by calling the agent's replay method.  We can then render the environment.  If the episode is over, we break from this loop, otherwise we continue with the next state being passed to the agent as the (now) current state.\n",
    "\n",
    "When the environment is set to have a 5x5 grid, the maximum reward is 8.  You'll find the training will converge to a rolling average of over 6 reward in about 10,000 episodes.  If all the code is implemented above, running the following cell unchanged should be sufficient.  In Google Colab, this takes about 15 minutes to run until completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eNF00A8ktvUs"
   },
   "outputs": [],
   "source": [
    "#-------------------- MAIN ----------------------------\n",
    "print('Setting up environment')\n",
    "env = Environment(5)\n",
    "\n",
    "state_dim = 4\n",
    "action_dim = 4 # left, right, up, down\n",
    "print('Setting up agent')\n",
    "MAX_EPSILON = 1 # the rate in which an agent randomly decides its action\n",
    "MIN_EPSILON = 0.05 # min rate in which an agent randomly decides its action\n",
    "LAMBDA = 0.00005      # speed of decay for epsilon\n",
    "num_episodes = 10000 # number of games we want the agent to play\n",
    "\n",
    "VERBOSE = False\n",
    "agent = Agent(state_dim, action_dim, lamb=LAMBDA,\n",
    "              max_epsilon=MAX_EPSILON, min_epsilon=MIN_EPSILON)\n",
    "env.reset()\n",
    "episode_rewards = []\n",
    "epsilons = []\n",
    "t0 = time.time()\n",
    "frames = []\n",
    "\n",
    "print('Running simulation')\n",
    "for episode in range(num_episodes):\n",
    "  s = env.reset() # Initial state\n",
    "  if episode % 1000 == 0:\n",
    "      fig = env.render()\n",
    "      frames.append(fig)\n",
    "  R = 0\n",
    "  while True: \n",
    "    a = agent.act(s, verbose=VERBOSE)\n",
    "\n",
    "    s_, r, done = env.step(a)\n",
    "\n",
    "    if done: # terminal state\n",
    "        s_ = None\n",
    "\n",
    "    agent.observe( (s, a, r, s_) )\n",
    "    agent.replay()\n",
    "\n",
    "    s = s_\n",
    "    R += r\n",
    "    \n",
    "    if episode % 1000 == 0:\n",
    "      fig = env.render()\n",
    "      frames.append(fig)\n",
    "    \n",
    "    if VERBOSE:\n",
    "      print(\"Action:\", a)\n",
    "      print(\"Reward:\", r)\n",
    "\n",
    "    if done:\n",
    "      break\n",
    "      \n",
    "  epsilons.append(agent.epsilon)\n",
    "  episode_rewards.append(R)\n",
    "  \n",
    "  if episode % 100 == 0:\n",
    "    print('Episode {}'.format(episode))\n",
    "    print('Time Elapsed: {0:.2f}s'.format(time.time() - t0))\n",
    "    print('Epsilon {}'.format(epsilons[-1]))\n",
    "    print('Last Episode Reward: {}'.format(R))\n",
    "    print('Episode Reward Rolling Mean: {}'.format(np.mean(episode_rewards[:-100])))\n",
    "    print('-'*10)\n",
    "\n",
    "agent.brain.model.save(\"brain.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eBiUaA2vSipn"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.title(\"Epsilon\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Epsilon value\")\n",
    "plt.plot(epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xjl4YwpS8WU"
   },
   "outputs": [],
   "source": [
    "smoothing = 50\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.title(\"Episode Reward\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"{} MA Reward\".format(smoothing))\n",
    "episode_ma = np.convolve(episode_rewards, \n",
    "                         np.ones((smoothing,))/smoothing, mode='valid')\n",
    "plt.plot(episode_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6I0iVz-MtXG-"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "To create animation, you need to install ffmpeg\n",
    "These lines will install it in Google Colab\n",
    "If you're running this notebook locally, you'll need to install\n",
    "ffmpeg on your computer: \n",
    "https://github.com/adaptlearning/adapt_authoring/wiki/Installing-FFmpeg\n",
    "\n",
    "Note: this lines will only work in Google Colab, do not run them locally.\n",
    "\"\"\"\n",
    "\n",
    "!apt install ffmpeg\n",
    "!which ffmpeg\n",
    "plt.rcParams['animation.ffmpeg_path'] = u'/usr/bin/ffmpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9ovkf9LMOZE"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell will compile the frames that should have been saved during training\n",
    "into an animation.  This required ffmpeg to be installed.\n",
    "\n",
    "If the main portion wasn't modified, this will have saved frames from every\n",
    "1,000 episode.  In the animation, you should see it start off performing poorly,\n",
    "but as it progresses it should perform optimally.\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.axis('off')\n",
    "l = ax.imshow(frames[0])\n",
    "\n",
    "def animate(i):\n",
    "    l.set_data(frames[i])\n",
    "\n",
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=12, metadata=dict(artist='Me'))\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(frames))\n",
    "\n",
    "ani.save('animation.mp4', writer=writer, dpi=220)\n",
    "time.sleep(5) # let it process (only necessary in Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xLRv4G5JB0y"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you're using Google Colab, you'll need to use the Google Colab\n",
    "download function to download both the model you trained and the animation\n",
    "you created.\n",
    "\n",
    "If you're not using Google Colab, those files should be saved in the directory\n",
    "where this notebook is located.\n",
    "\"\"\"\n",
    "\n",
    "# To Save Brain\n",
    "files.download(\"brain.h5\")\n",
    "\n",
    "# To Save Animation\n",
    "files.download('animation.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEVjv7A-HFPq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
